{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.pipeline import EntityRuler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'./DatabaseTableUpload/Appendix_A_Capstone_DataSharingProposal.xlsx', sheet_name='A.25_ServiceNow_Incidents')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Named Entity Recognition (NER)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sizes: ['sm', 'md', 'lg']<br>\n",
    "Your pipeline must be compatable with your current version of SpaCy.\n",
    "\n",
    "Can download the following on Conda (base) environment:\n",
    "`pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_[SIZE]-[VERSION]/en_core_web_md-[VERSION].tar.gz`\n",
    "\n",
    "Alternatively, in Python:\n",
    "`python -m spacy download en_core_web_sm`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Instantiate a Tokenizer with the default settings for English, including punctuation rules and exceptions.\n",
    "tokenizer = nlp.tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_trf') # Load transfomer model.\n",
    "print(nlp.pipe_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Enhance the transformer NER model with added examples.\n",
    "#### https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_test = \"Microsoft Excel throws error 0xC0000142\"\n",
    "phrase_len = len(\"Microsoft Outlook\")\n",
    "text_test[0:15]\n",
    "print(phrase_len)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training examples - do this only for *individual* NERs, e.g. single-word NERs.\n",
    "train_data = [\n",
    "    (\"Windows Defender blocked my internet access this morning, can you take a look?\", {\"entities\": [(0, 16, \"SOFTWARE\")]}),\n",
    "    (\"BitLocker malfunctioned upon scanning this morning\", {\"entities\": [(0,9, \"SOFTWARE\")]}),\n",
    "    (\"Please check to see if Windows Firewall is working\", {\"entities\": [(23,39, \"SOFTWARE\")]}),\n",
    "    (\"Baird TrustDesk Migration to OneDrive - Error migrating\", {\"entities\": [(0, 15, \"SECURITY\"), (29, 37, \"SOFTWARE\")]}),\n",
    "    (\"Hello. My name is spelled incorrectly on the DocuSign application. It is Veronica Fitzpatrick, some letters are all flipped around  backwards and I was hoping to have that fixed!\", {\"entities\": [(45, 53, \"SOFTWARE\"), (73, 93, \"PERSON\")]}),\n",
    "    (\"I use Microsoft Office for my daily work\", {\"entities\": [(6, 22, \"SOFTWARE\")]}),\n",
    "    (\"Microsoft Teams is crashing\", {\"entities\": [(0, 15, \"SOFTWARE\")]}),\n",
    "    (\"Microsoft Outlook is crashing\", {\"entities\": [(0, 17, \"SOFTWARE\")]}),\n",
    "    (\"Microsoft Excel throws error 0xC0000142\", {\"entities\": [(0, 15, \"SOFTWARE\")]})\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "# Disable pipeline components you don't need to change.\n",
    "\n",
    "# METHOD 1.\n",
    "# other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "# with nlp.select_pipes(disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "#     batches = spacy.util.minibatch(train_data, size=2)\n",
    "#     for batch in batches:\n",
    "#         for text, annotations in batch:\n",
    "#             # Create Example.\n",
    "#             doc = nlp.make_doc(text)\n",
    "#             example = Example.from_dict(doc, annotations)\n",
    "#             # Update the model.\n",
    "#             nlp.update([example], drop=0.3)\n",
    "\n",
    "# METHOD 2.\n",
    "# pipe_exceptions = [\"transformer\", \"ner\"]\n",
    "# unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "# with nlp.select_pipes(disable=unaffected_pipes):\n",
    "#     for itn in range(20): # Set iterations.\n",
    "#         random.shuffle(train_data)\n",
    "#         losses = {}\n",
    "#         for text, annotations in train_data:\n",
    "#             doc = nlp.make_doc(text)\n",
    "#             example = Example.from_dict(doc, annotations)\n",
    "#             nlp.update([example], losses=losses)\n",
    "#         print(\"Iteration:\", itn + 1, \"Loss:\", losses)\n",
    "#\n",
    "#\n",
    "# METHOD 3.\n",
    "# if \"SOFTWARE\" not in ner.labels:\n",
    "#     ner.add_label(\"SOFTWARE\")\n",
    "#\n",
    "# pipe_exceptions = [\"transformer\", \"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "# unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "#\n",
    "# with nlp.select_pipes(disable=unaffected_pipes):\n",
    "#     optimizer = nlp.resume_training()\n",
    "#     for iteration in range(10):  # Adjust the number of iterations as needed\n",
    "#         losses = {}\n",
    "#         for text, annotations in train_data:\n",
    "#             doc = nlp.make_doc(text)\n",
    "#             example = Example.from_dict(doc, annotations)\n",
    "#             nlp.update([example], losses=losses, sgd=optimizer)\n",
    "#         print(\"Iteration:\", iteration + 1, \"Loss:\", losses)\n",
    "\n",
    "# METHOD 4.\n",
    "from spacy.util import minibatch\n",
    "pipe_exceptions = [\"transformer\", \"ner\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "with nlp.select_pipes(disable=unaffected_pipes):\n",
    "    for itn in range(20): # Set iterations.\n",
    "        random.shuffle(train_data)\n",
    "        batches = minibatch(train_data, size=2)\n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            examples = []\n",
    "            for text, annotations in train_data:\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                examples.append(example)\n",
    "            nlp.update(examples=examples, losses=losses, drop=0.4)\n",
    "        print(\"Iteration:\", itn + 1, \"Loss:\", losses)\n",
    "\n",
    "nlp.to_disk(\"./en_web_core_trf_updated\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"./en_web_core_trf_updated\")\n",
    "# cust_nlp = spacy.load(\"en_web_core_trf_updated\")\n",
    "# cust_nlp.replace_listeners(tok2vec_name=\"transformer\", pipe_name=\"ner\", listeners=[\"model.tok2vec\"])\n",
    "# nlp.add_pipe(factory_name=\"ner\", name=\"ner_custom\", source=cust_nlp, before=\"ner\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(nlp.pipe_names)\n",
    "print(nlp.get_pipe(\"ner\").labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for text, _ in train_data:\n",
    "    doc = nlp(text)\n",
    "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Using EntityRuler() to directly classify new entities based on a set of rules.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# config = {\"overwrite_ents\": True}\n",
    "# nlp.add_pipe(\"entity_ruler\", config=config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rulerSoftwares = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "patterns = [\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"Windows\"},\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"Windows Defender\"},\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"Windows OneDrive\"},\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"BitLocker\"},\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"BitDefender\"},\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"OneDrive\"},\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"Windows Firewall\"},\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"Windows Server\"},\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"Microsoft Teams\"},\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"Microsoft\"},\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"DocuSign\"}\n",
    "]\n",
    "rulerSoftwares.add_patterns(patterns)\n",
    "print(nlp.pipe_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_web_core_trf_updated\")\n",
    "doc = nlp(\"DocuSigns is doing its job.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Example of token generation for the first body of text.\n",
    "doc = nlp(df['short_description'][0])\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_) # Print: token, POS, syntactic dependency."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create function to add an article's tokens to `doc_list`.\n",
    "# Tokenize one time, then use that object for the subsequent accumulators.\n",
    "# Returns None many times.\n",
    "doc_list = []\n",
    "def to_doc_list(text):\n",
    "    doc_list.append(nlp(text))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Takes time to generate tokens from each cell's fulltext.\n",
    "df['short_description'].apply(to_doc_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assign `doc_list` to `doc_series` as a Series object.\n",
    "doc_series = pd.Series(doc_list)\n",
    "doc_series"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for doc in doc_series:\n",
    "    print(doc)\n",
    "    filtered_string = \"\"\n",
    "    for token in doc:\n",
    "        if token.ent_type_ in ['PERSON', 'PRODUCT', 'MONEY', 'CARDINAL', 'QUANTITY', 'PERCENT', 'SOFTWARE', 'SECURITY']:\n",
    "            new_token = \" <{}>\".format(token.ent_type_)\n",
    "        # elif token.pos_ in ['PROPN']:\n",
    "        #     new_token = \" <PROPN>\"\n",
    "        # elif token.pos_ in ['PROPN', 'NUM']:\n",
    "        #     new_token = \" <{}>\".format(token.ent_type_)\n",
    "        elif token.pos_ == \"PUNCT\":\n",
    "            new_token = token.text\n",
    "        else:\n",
    "            new_token = \" {}\".format(token.text)\n",
    "        filtered_string += new_token\n",
    "    filtered_string = filtered_string[1:]\n",
    "    print(filtered_string, '\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove phone numbers.\n",
    "import re\n",
    "def remove_phone_numbers(text):\n",
    "    pattern = r\"\\b(?:\\+?\\d{1,3}[-.])?\\(?\\d{1,3}\\)?[-.\\s]?\\d{1,3}[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9}\\b\"\n",
    "    return re.sub(pattern, \"<PHONE NUMBER>\", text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['short_description'].apply(remove_phone_numbers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove email addresses.\n",
    "def remove_email_addresses(text):\n",
    "    pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\"\n",
    "    return re.sub(pattern, \"<EMAIL>\", text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Check NER results.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Dictionary accumulator for entities based on entity type\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ent_dict = {}\n",
    "def count_ent(doc):\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ not in ent_dict:\n",
    "            ent_dict[ent.label_] = 1\n",
    "        else:\n",
    "            ent_dict[ent.label_]+=1\n",
    "doc_series.apply(count_ent)\n",
    "ent_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(nlp.get_pipe(\"ner\").labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### PERSON.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ent_dict = {}\n",
    "def count_person(doc):\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            if ent.text not in ent_dict:\n",
    "                ent_dict[ent.text]=1\n",
    "            else:\n",
    "                ent_dict[ent.text]+=1\n",
    "doc_series.apply(count_person)\n",
    "sorted(ent_dict.items(), key=lambda x: x[1], reverse=True)[0:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "&nbsp;\n",
    "#### Save out above entity list + counts as a pd.Sereies() object.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_dir = r'./PII'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ner_obj = sorted(ent_dict.items(), key=lambda x: x[1], reverse=True) # specify which dict to save\n",
    "ner_obj = pd.Series(ner_obj)\n",
    "ner_obj.to_csv(output_dir + 'ner_obj.csv', sep=';', encoding='utf-8-sig')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
