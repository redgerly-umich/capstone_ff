{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Config Manager data to the Azure Database\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "-  Python dependencies:\n",
    "    - pandas : 2.0.0,\n",
    "    - json   : 2.0.9,\n",
    "    - pyarrow: 12.0.1,\n",
    "    - pyodbc : 4.0.39,\n",
    "    - azure  : 5.0.0\n",
    "- `fun.json` for access credentials\n",
    "- `/create_sql` folder for SQL code to create tables\n",
    "\n",
    "I assigned us all an equal number of tables to load in the Historic Config Manager Summary excel document in our shared google drive. The names of the parquet files in Azure Blob Storage are also included there.\n",
    "See the link below: https://docs.google.com/spreadsheets/d/1GTahzraMPXh9RTNQHnqzBrZaee4qBTkjZV6v58WjhfM/edit?usp=sharing\n",
    "\n",
    "\n",
    "\n",
    "### Section 1 - Workflow Instructions \n",
    "\n",
    "1. Assemble prerequisites. Ensure the python dependencies listed above are installed in your working environment. Make a subdirectory in your working directory called `/assets` and place the `fun.json` file there.\n",
    "\n",
    "2. Download the config manager parquet files from Azure Blob Storage. Use the code in section 2. Credentials needed to access the storage container are provided in `fun.json`. \n",
    "\n",
    "3. Create the SQL table in the Azure database for each table. Connect to the Azure SQL Database using DBeaver or another preferred database development environment. Run the SQL code for the corresponding table in `/assets/create_sql` to create each config manager database table. \n",
    "\n",
    "4. Load the SQL table with data from the parquet file. Use the code in section 3. Feel free to optimize this code to make the load process faster!\n",
    "\n",
    "5. Repeat steps 2 through 4 for all Config manager tables that were assigned.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 - Download parquet files from Azure Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T13:42:12.818037764Z",
     "start_time": "2023-07-07T13:42:12.199502777Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import pyodbc\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Read in credentials\n",
    "with open('assets/fun.json') as f:\n",
    "    cred = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T13:42:12.821348009Z",
     "start_time": "2023-07-07T13:42:12.818756616Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes 2.1784846782684326 seconds to download Persist_WorkstationStatus_Data.parquet\n"
     ]
    }
   ],
   "source": [
    "def download_blob(blob_name, local_file_name):\n",
    "    '''\n",
    "    Downloads the parquet file from the blob container.\n",
    "    '''\n",
    "\n",
    "    # Declare variables\n",
    "    STORAGEACCOUNTURL= cred['in_the_sun']\n",
    "    STORAGEACCOUNTKEY= cred['fun']\n",
    "    LOCALFILENAME= local_file_name\n",
    "    CONTAINERNAME= 'configmanagertest1'\n",
    "    BLOBNAME= blob_name\n",
    "\n",
    "    # Download from blob\n",
    "    t1=time.time()\n",
    "    blob_service_client_instance = BlobServiceClient(account_url=STORAGEACCOUNTURL, credential=STORAGEACCOUNTKEY)\n",
    "    blob_client_instance = blob_service_client_instance.get_blob_client(CONTAINERNAME, BLOBNAME, snapshot=None)\n",
    "    with open(LOCALFILENAME, \"wb\") as my_blob:\n",
    "        blob_data = blob_client_instance.download_blob()\n",
    "        blob_data.readinto(my_blob)\n",
    "    t2=time.time()\n",
    "    print((\"It takes %s seconds to download \"+BLOBNAME) % (t2 - t1))\n",
    "\n",
    "# Call the function\n",
    "download_blob(blob_name='Persist_WorkstationStatus_Data.parquet', local_file_name='assets/WorkstationStatusData.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T13:42:15.016555845Z",
     "start_time": "2023-07-07T13:42:12.823537121Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "         RWB_WorkstationStatus_Data_ID  RWB_ETL_EVENT_DESTINATION_IDENTIFIER   \n0                                  310                                   879  \\\n1                                  311                                   879   \n2                                  312                                   879   \n3                                  313                                   879   \n4                                  314                                   879   \n...                                ...                                   ...   \n1071355                        1257955                                 10723   \n1071356                        1257956                                 10723   \n1071357                        1257957                                 10723   \n1071358                        1257958                                 10723   \n1071359                        1257959                                 10723   \n\n               RWB_CREATE_TIMESTAMP RWB_EFFECTIVE_DATE  MachineID   \n0        2023-01-19 09:35:25 -06:00         2023-01-19   16783564  \\\n1        2023-01-19 09:35:25 -06:00         2023-01-19   16783835   \n2        2023-01-19 09:35:25 -06:00         2023-01-19   16784339   \n3        2023-01-19 09:35:25 -06:00         2023-01-19   16784657   \n4        2023-01-19 09:35:25 -06:00         2023-01-19   16785825   \n...                             ...                ...        ...   \n1071355  2023-07-02 08:03:25 -05:00         2023-07-02   16819021   \n1071356  2023-07-02 08:03:25 -05:00         2023-07-02   16819022   \n1071357  2023-07-02 08:03:25 -05:00         2023-07-02   16819023   \n1071358  2023-07-02 08:03:25 -05:00         2023-07-02   16819024   \n1071359  2023-07-02 08:03:25 -05:00         2023-07-02   16819025   \n\n         InstanceKey  RevisionID  AgentID             TimeKey   \n0                  5          27        1 2023-01-18 14:44:10  \\\n1                  6           2        1 2023-01-18 12:45:55   \n2                  4          29        1 2023-01-19 05:44:24   \n3                  4          29        1 2023-01-19 06:33:40   \n4                  3          29        1 2023-01-18 05:28:00   \n...              ...         ...      ...                 ...   \n1071355            2           3        1 2023-07-02 03:15:40   \n1071356            1           1        1 2023-06-30 14:28:00   \n1071357            1           4        1 2023-07-02 07:48:11   \n1071358            1           3        1 2023-07-02 04:31:28   \n1071359            1           2        1 2023-07-02 06:22:35   \n\n                                  rowversion          LastHWScan   \n0              b'\\x00\\x00\\x00\\x01\\x1b$\\xe8\"' 2023-01-18 14:43:36  \\\n1        b'\\x00\\x00\\x00\\x01\\x1b\\x1d\\xd7\\xcc' 2023-01-18 12:44:59   \n2                 b'\\x00\\x00\\x00\\x01\\x1bN=@' 2023-01-19 05:43:05   \n3              b'\\x00\\x00\\x00\\x01\\x1bP\\xd3`' 2023-01-19 06:31:46   \n4           b'\\x00\\x00\\x00\\x01\\x1a\\xf9q\\xfd' 2023-01-18 05:26:20   \n...                                      ...                 ...   \n1071355           b'\\x00\\x00\\x00\\x01ByE\\x0f' 2023-07-02 02:53:04   \n1071356           b'\\x00\\x00\\x00\\x01B8\\x80c' 2023-06-30 12:26:27   \n1071357           b'\\x00\\x00\\x00\\x01B~*\\x88' 2023-07-02 05:46:22   \n1071358           b'\\x00\\x00\\x00\\x01Bzs\\x08' 2023-07-02 02:30:37   \n1071359           b'\\x00\\x00\\x00\\x01B|\\xbe2' 2023-07-02 07:20:52   \n\n         SystemDefaultLCID  TimezoneOffset LastReportVersion  \n0                     1033            -360       30064771098  \n1                     1033            -360       34359738369  \n2                     1033            -360       30064771100  \n3                     1033            -360       25769803804  \n4                     1033            -360       21474836508  \n...                    ...             ...               ...  \n1071355               1033            -300        8589934594  \n1071356               1033            -420        4294967296  \n1071357               1033            -420        4294967299  \n1071358               1033            -420        4294967298  \n1071359               1033            -240       12884901889  \n\n[1071360 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RWB_WorkstationStatus_Data_ID</th>\n      <th>RWB_ETL_EVENT_DESTINATION_IDENTIFIER</th>\n      <th>RWB_CREATE_TIMESTAMP</th>\n      <th>RWB_EFFECTIVE_DATE</th>\n      <th>MachineID</th>\n      <th>InstanceKey</th>\n      <th>RevisionID</th>\n      <th>AgentID</th>\n      <th>TimeKey</th>\n      <th>rowversion</th>\n      <th>LastHWScan</th>\n      <th>SystemDefaultLCID</th>\n      <th>TimezoneOffset</th>\n      <th>LastReportVersion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>310</td>\n      <td>879</td>\n      <td>2023-01-19 09:35:25 -06:00</td>\n      <td>2023-01-19</td>\n      <td>16783564</td>\n      <td>5</td>\n      <td>27</td>\n      <td>1</td>\n      <td>2023-01-18 14:44:10</td>\n      <td>b'\\x00\\x00\\x00\\x01\\x1b$\\xe8\"'</td>\n      <td>2023-01-18 14:43:36</td>\n      <td>1033</td>\n      <td>-360</td>\n      <td>30064771098</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>311</td>\n      <td>879</td>\n      <td>2023-01-19 09:35:25 -06:00</td>\n      <td>2023-01-19</td>\n      <td>16783835</td>\n      <td>6</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2023-01-18 12:45:55</td>\n      <td>b'\\x00\\x00\\x00\\x01\\x1b\\x1d\\xd7\\xcc'</td>\n      <td>2023-01-18 12:44:59</td>\n      <td>1033</td>\n      <td>-360</td>\n      <td>34359738369</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>312</td>\n      <td>879</td>\n      <td>2023-01-19 09:35:25 -06:00</td>\n      <td>2023-01-19</td>\n      <td>16784339</td>\n      <td>4</td>\n      <td>29</td>\n      <td>1</td>\n      <td>2023-01-19 05:44:24</td>\n      <td>b'\\x00\\x00\\x00\\x01\\x1bN=@'</td>\n      <td>2023-01-19 05:43:05</td>\n      <td>1033</td>\n      <td>-360</td>\n      <td>30064771100</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>313</td>\n      <td>879</td>\n      <td>2023-01-19 09:35:25 -06:00</td>\n      <td>2023-01-19</td>\n      <td>16784657</td>\n      <td>4</td>\n      <td>29</td>\n      <td>1</td>\n      <td>2023-01-19 06:33:40</td>\n      <td>b'\\x00\\x00\\x00\\x01\\x1bP\\xd3`'</td>\n      <td>2023-01-19 06:31:46</td>\n      <td>1033</td>\n      <td>-360</td>\n      <td>25769803804</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>314</td>\n      <td>879</td>\n      <td>2023-01-19 09:35:25 -06:00</td>\n      <td>2023-01-19</td>\n      <td>16785825</td>\n      <td>3</td>\n      <td>29</td>\n      <td>1</td>\n      <td>2023-01-18 05:28:00</td>\n      <td>b'\\x00\\x00\\x00\\x01\\x1a\\xf9q\\xfd'</td>\n      <td>2023-01-18 05:26:20</td>\n      <td>1033</td>\n      <td>-360</td>\n      <td>21474836508</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1071355</th>\n      <td>1257955</td>\n      <td>10723</td>\n      <td>2023-07-02 08:03:25 -05:00</td>\n      <td>2023-07-02</td>\n      <td>16819021</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2023-07-02 03:15:40</td>\n      <td>b'\\x00\\x00\\x00\\x01ByE\\x0f'</td>\n      <td>2023-07-02 02:53:04</td>\n      <td>1033</td>\n      <td>-300</td>\n      <td>8589934594</td>\n    </tr>\n    <tr>\n      <th>1071356</th>\n      <td>1257956</td>\n      <td>10723</td>\n      <td>2023-07-02 08:03:25 -05:00</td>\n      <td>2023-07-02</td>\n      <td>16819022</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2023-06-30 14:28:00</td>\n      <td>b'\\x00\\x00\\x00\\x01B8\\x80c'</td>\n      <td>2023-06-30 12:26:27</td>\n      <td>1033</td>\n      <td>-420</td>\n      <td>4294967296</td>\n    </tr>\n    <tr>\n      <th>1071357</th>\n      <td>1257957</td>\n      <td>10723</td>\n      <td>2023-07-02 08:03:25 -05:00</td>\n      <td>2023-07-02</td>\n      <td>16819023</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2023-07-02 07:48:11</td>\n      <td>b'\\x00\\x00\\x00\\x01B~*\\x88'</td>\n      <td>2023-07-02 05:46:22</td>\n      <td>1033</td>\n      <td>-420</td>\n      <td>4294967299</td>\n    </tr>\n    <tr>\n      <th>1071358</th>\n      <td>1257958</td>\n      <td>10723</td>\n      <td>2023-07-02 08:03:25 -05:00</td>\n      <td>2023-07-02</td>\n      <td>16819024</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2023-07-02 04:31:28</td>\n      <td>b'\\x00\\x00\\x00\\x01Bzs\\x08'</td>\n      <td>2023-07-02 02:30:37</td>\n      <td>1033</td>\n      <td>-420</td>\n      <td>4294967298</td>\n    </tr>\n    <tr>\n      <th>1071359</th>\n      <td>1257959</td>\n      <td>10723</td>\n      <td>2023-07-02 08:03:25 -05:00</td>\n      <td>2023-07-02</td>\n      <td>16819025</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2023-07-02 06:22:35</td>\n      <td>b'\\x00\\x00\\x00\\x01B|\\xbe2'</td>\n      <td>2023-07-02 07:20:52</td>\n      <td>1033</td>\n      <td>-240</td>\n      <td>12884901889</td>\n    </tr>\n  </tbody>\n</table>\n<p>1071360 rows Ã— 14 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in parquet file as DataFrame.\n",
    "df = pd.read_parquet('./assets/WorkstationStatusData.parquet', engine='pyarrow')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T13:42:15.345810110Z",
     "start_time": "2023-07-07T13:42:15.020038929Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Replace empty strings with None.\n",
    "df = df.applymap(lambda x: None if x == \"\" else x)\n",
    "count = (df.eq(\"\")).sum().sum()\n",
    "print(\"Number of empty strings:\", count)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-07-07T13:42:15.367195303Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Replace any np.inf values with np.nan.\n",
    "df.replace({np.inf: np.nan, -np.inf: np.nan}, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pq = pq.where(pd.notnull(pq), None)\n",
    "# pq.fillna(0, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert float64 columns to int.\n",
    "float_columns = df.select_dtypes(include=['float64']).columns\n",
    "df[float_columns] = df[float_columns].fillna(0)\n",
    "df[float_columns] = df[float_columns].astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save out parquet file.\n",
    "df.to_parquet('./assets/WorkstationStatusCorrected.parquet', engine='pyarrow')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 - Load data to the Azure SQL table"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Make the database table.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Connect to database.\n",
    "1. Connect using SQL editor of choice (DBeaver, DataGrip).\n",
    "2. Select Azure SQL using built-in connection type.\n",
    "3. In pop-up dialog window, enter connection credentials using values from `fun.json` to authenticate.\n",
    "4. Connect to Azure DB.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Add table.\n",
    "1. Find `Tables` subdirectory under `dbo` schema.\n",
    "2. Right-click to open context window. Highlight \"Add...\". Click \"New table\".\n",
    "3. Dialog window appears. Provide SQL template code from `create_sql` directory for the corresponding table.\n",
    "    - Change table name in dialog window to `Persist.Table_Name`.\n",
    "    - You may need to edit the SQL to successfully create the correct table.\n",
    "    - Remove `IDENTITY(1,1)`\n",
    "    - Remove `ON [filegroup]` statement from `WITH` near bottom.\n",
    "    - Change table name to: `dbo.[Persist.Table_Name]`\n",
    "4. Once the table is created, run the below function to push your data to the table.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Upload the DataFrame as a parquet file to the table.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Define the conn string\n",
    "conn_string = f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={cred['server']};DATABASE={cred['db']};UID={cred['uid']};PWD={cred['pwd']}\"\n",
    "# Establish a connection to the SQL Server database\n",
    "\n",
    "conn = pyodbc.connect(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_table_v2(conn_obj, table_name, pq_file_name):\n",
    "    '''\n",
    "    Loads a full table of data into the table_name of the\n",
    "    connect database in the conn_obj. Uses chunking for an\n",
    "    efficient load.\n",
    "    '''\n",
    "\n",
    "    # Create a cursor object\n",
    "    cursor = conn_obj.cursor()\n",
    "\n",
    "    # Read in the parquet file\n",
    "    parquet_data = pq.ParquetFile(pq_file_name)\n",
    "\n",
    "    # Get the field names from the parquet file schema\n",
    "    pq_field_names = parquet_data.schema_arrow.names\n",
    "    col_tup_str_curr = '(' + ','.join(pq_field_names) +')'\n",
    "\n",
    "    # Get the number of (?) to duplicate in the insert query\n",
    "    num_qs_rep = len(col_tup_str_curr.split(','))\n",
    "\n",
    "    # Make the values string\n",
    "    values_ques_str = ','.join(tuple(['?']*num_qs_rep))\n",
    "\n",
    "    # Define the SQL insert statement\n",
    "    insert_query = \"INSERT INTO \" + table_name + \" \" + col_tup_str_curr + \" VALUES (\" + values_ques_str + \")\"\n",
    "\n",
    "    # Read the Parquet file in chunks\n",
    "    chunk_size = 10000\n",
    "    num_rows = parquet_data.metadata.num_rows\n",
    "    num_chunks = num_rows // chunk_size + 1\n",
    "\n",
    "    # Track the rows loaded during the insert\n",
    "    rows_loaded = 0\n",
    "    track_to_100k = 0\n",
    "\n",
    "    # Process and insert the data in chunks\n",
    "    for batch in parquet_data.iter_batches(chunk_size):\n",
    "\n",
    "        # Read the chunk of data from Parquet\n",
    "        chunk = batch.to_pandas()\n",
    "\n",
    "        # Convert the chunk to a list of tuples\n",
    "        records = [tuple(row) for row in chunk.itertuples(index=False)]\n",
    "\n",
    "        # Execute the INSERT statement with executemany\n",
    "        cursor.executemany(insert_query, records)\n",
    "        conn_obj.commit()\n",
    "\n",
    "        # Notify when 100K rows are loaded\n",
    "        rows_loaded = rows_loaded + len(chunk)\n",
    "        track_to_100k = len(chunk) + track_to_100k\n",
    "        if track_to_100k >= 100000:\n",
    "            print(f\"{rows_loaded} number of rows loaded.\")\n",
    "            track_to_100k = 0\n",
    "\n",
    "    # Close the cursor and connection\n",
    "    cursor.close()\n",
    "\n",
    "# Call the function\n",
    "load_table_v2(conn_obj=conn, table_name='dbo.[Persist.WorkstationStatus_DATA]', pq_file_name='assets/WorkstationStatusCorrected.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Document dependencies in a jupyter notebook\n",
    "# Dependencies for this notebook\n",
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark --iversions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
